# -*- coding: utf-8 -*-
"""polars_stats.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eKVcR0dVoDSFTNd4t-5AvNGqAweNsSBo
"""

import polars as pl
import pandas as pd
import os
import csv
from google.colab import drive
import math
from collections import Counter, defaultdict
import os

drive.mount('/content/drive')

fb_posts = '/content/drive/My Drive/Presidential Election Data/2024_fb_posts_president_scored_anon.csv'
fb_ads = '/content/drive/My Drive/Presidential Election Data/2024_fb_ads_president_scored_anon.csv'
tw_posts = '/content/drive/My Drive/Presidential Election Data/2024_tw_posts_president_scored_anon.csv'

def analyze_polars_dataset(df: pl.DataFrame):
    print("===== General Dataset Summary =====\n")

    for col in df.columns:
        col_data = df[col]
        print(f"Column: {col}")
        non_null = col_data.drop_nulls()

        if col_data.dtype.is_numeric():
            print(f"  Count: {non_null.len()}")
            print(f"  Mean: {non_null.mean()}")
            print(f"  Min: {non_null.min()}")
            print(f"  Max: {non_null.max()}")
            print(f"  Stddev: {non_null.std()}")
        else:
            unique_vals = non_null.unique()
            print(f"  Count: {non_null.len()}")
            print(f"  Unique values: {unique_vals.len()}")

            if not non_null.is_empty():
                freq_df = non_null.value_counts().sort("count", descending=True)
                top_val = freq_df[0][col].item()       # get raw string
                top_count = freq_df[0]["count"].item() # get count as int
                print(f"  Most frequent: {top_val} (appears {top_count} times)")

        print()

df_ads = pl.read_csv(fb_ads)

analyze_polars_dataset(df_ads)

import polars as pl

def analyze_polars_grouped(df: pl.DataFrame, group_cols: list[str]):
    print(f"\n===== Grouped Analysis by {group_cols} =====\n")

    # Get a list of other columns (non-grouping)
    other_cols = [col for col in df.columns if col not in group_cols]

    # Group the dataframe
    grouped = df.group_by(group_cols)

    # Loop over each group - limit to first 3
    for i, (group_key, group_df) in enumerate(grouped):
        if i >= 3:
            break

        print(f"Group: {group_key}")

        for col in other_cols:
            col_data = group_df[col].drop_nulls()

            print(f"  Column: {col}")
            if col_data.is_empty():
                print("     No data")
                continue

            if col_data.dtype.is_numeric():
                print(f"    Count: {col_data.len()}")
                print(f"    Mean: {col_data.mean()}")
                print(f"    Min: {col_data.min()}")
                print(f"    Max: {col_data.max()}")
                print(f"    Stddev: {col_data.std()}")
            else:
                unique_vals = col_data.unique()
                print(f"    Count: {col_data.len()}")
                print(f"    Unique values: {unique_vals.len()}")
                # Get top value cleanly
                freq_df = col_data.value_counts().sort("count", descending=True)
                top_val = freq_df[0][col].item()
                top_count = freq_df[0]["count"].item()
                print(f"    Most frequent: {top_val} (appears {top_count} times)")
        print("-" * 60)

analyze_polars_grouped(df_ads, ['page_id'])
analyze_polars_grouped(df_ads, ['page_id', 'ad_id'])

